{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import time\n",
    "import math\n",
    "from matplotlib import pyplot\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# S is the source sequence length\n",
    "# T is the target sequence length\n",
    "# N is the batch size\n",
    "# E is the feature number\n",
    "\n",
    "#src = torch.rand((10, 32, 512)) # (S,N,E) \n",
    "#tgt = torch.rand((20, 32, 512)) # (T,N,E)\n",
    "#out = transformer_model(src, tgt)\n",
    "input_window = 100 # number of input steps\n",
    "output_window = 1 # number of prediction steps, in this model its fixed to one\n",
    "block_len = input_window + output_window # for one input-output pair\n",
    "batch_size = 10\n",
    "train_size = 0.8\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()       \n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        # div_term = torch.exp(\n",
    "        #     torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model)\n",
    "        # )\n",
    "        div_term = 1 / (10000 ** ((2 * np.arange(d_model)) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term[0::2])\n",
    "        pe[:, 1::2] = torch.cos(position * div_term[1::2])\n",
    "\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1) # [5000, 1, d_model],so need seq-len <= 5000\n",
    "        #pe.requires_grad = False\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # print(self.pe[:x.size(0), :].repeat(1,x.shape[1],1).shape ,'---',x.shape)\n",
    "        # dimension 1 maybe inequal batchsize\n",
    "        return x + self.pe[:x.size(0), :].repeat(1,x.shape[1],1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def layer_init(layer):\n",
    "    for name,param in layer.named_parameters():\n",
    "        if 'weight' in name and param.data.dim()==2:\n",
    "            nn.init.kaiming_normal_(param.data)\n",
    "        if 'bias' in name:\n",
    "            nn.init.constant_(param,0.0)\n",
    "\n",
    "class TransAm(nn.Module):\n",
    "    def __init__(self,feature_size=250,encoder_num_layers=1,decoder_num_layers=1,dropout=0.1):\n",
    "        super(TransAm, self).__init__()\n",
    "        self.model_type = 'Transformer'\n",
    "        self.input_embedding  = nn.Linear(1,feature_size)\n",
    "        self.src_mask = None\n",
    "\n",
    "        self.pos_encoder = PositionalEncoding(feature_size)\n",
    "        self.encoder_layer = nn.TransformerEncoderLayer(d_model=feature_size, nhead=10, dropout=dropout)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(self.encoder_layer, num_layers=encoder_num_layers)\n",
    "        self.decoder_layer = nn.TransformerDecoderLayer(d_model=feature_size, nhead=10, dropout=dropout)\n",
    "        self.decoder = nn.TransformerDecoder(self.decoder_layer, num_layers=decoder_num_layers)\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        initrange = 0.1    \n",
    "        self.input_embedding.weight.data.uniform_(-initrange, initrange)\n",
    "        self.input_embedding.bias.data.zero_()\n",
    "        layer_init(self.encoder_layer)\n",
    "        layer_init(self.decoder_layer)\n",
    "\n",
    "    def forward(self,src,target):\n",
    "        # src with shape (input_window, batch_len, 1)\n",
    "        if self.src_mask is None or self.src_mask.size(0) != len(src):\n",
    "            device = src.device\n",
    "            mask = self._generate_square_subsequent_mask(len(src)).to(device)\n",
    "            self.src_mask = mask\n",
    "\n",
    "        src = self.input_embedding(src)\n",
    "        src = self.pos_encoder(src)\n",
    "        memory = self.transformer_encoder(src, self.src_mask)\n",
    "\n",
    "        output = self.decoder(\n",
    "            tgt = self.input_embedding(target),\n",
    "            memory = memory,\n",
    "            tgt_mask = self.src_mask\n",
    "        )\n",
    "        return output\n",
    "\n",
    "    def _generate_square_subsequent_mask(self, sz):\n",
    "        mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\n",
    "        mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
    "        return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if window is 100 and prediction step is 1\n",
    "# in -> [0..99]\n",
    "# target -> [1..100]\n",
    "'''\n",
    "In fact, assuming that the number of samples is N, \n",
    "the length of the input sequence is m, and the backward prediction is k steps, \n",
    "then length of a block [input : 1 , 2 ... m  -> output : k , k+1....m+k ] \n",
    "should be (m+k) :  block_len, so to ensure that each block is complete, \n",
    "the end element of the last block should be the end element of the entire sequence, \n",
    "so the actual number of blocks is [N - block_len + 1] \n",
    "'''\n",
    "def create_inout_sequences(input_data, input_window ,output_window):\n",
    "    inout_seq = []\n",
    "    L = len(input_data)\n",
    "    block_num =  L - block_len + 1\n",
    "    # total of [N - block_len + 1] blocks\n",
    "    # where block_len = input_window + output_window\n",
    "\n",
    "    for i in range( block_num ):\n",
    "        train_seq = input_data[i : i + input_window]\n",
    "        train_label = input_data[i + output_window : i + input_window + output_window]\n",
    "        inout_seq.append((train_seq ,train_label))\n",
    "\n",
    "    return torch.FloatTensor(np.array(inout_seq))\n",
    "\n",
    "def get_data():\n",
    "    # construct a littel toy dataset\n",
    "    time        = np.arange(0, 400, 0.1)    \n",
    "    amplitude   = np.sin(time) + np.sin(time * 0.05) + \\\n",
    "                  np.sin(time * 0.12) * np.random.normal(-0.2, 0.2, len(time))\n",
    "\n",
    "\n",
    "    \n",
    "    #loading weather data from a file\n",
    "    #from pandas import read_csv\n",
    "    #series = read_csv('daily-min-temperatures.csv', header=0, index_col=0, parse_dates=True, squeeze=True)\n",
    "    \n",
    "    # looks like normalizing input values curtial for the model\n",
    "    scaler = MinMaxScaler(feature_range=(-1, 1)) \n",
    "    #amplitude = scaler.fit_transform(series.to_numpy().reshape(-1, 1)).reshape(-1)\n",
    "    amplitude = scaler.fit_transform(amplitude.reshape(-1, 1)).reshape(-1)\n",
    "\n",
    "    sampels = int(len(time) * train_size) # use a parameter to control training size\n",
    "    train_data = amplitude[:sampels]\n",
    "    test_data = amplitude[sampels:]\n",
    "\n",
    "    # convert our train data into a pytorch train tensor\n",
    "    #train_tensor = torch.FloatTensor(train_data).view(-1)\n",
    "\n",
    "    train_sequence = create_inout_sequences( train_data,input_window ,output_window)\n",
    "    '''\n",
    "    train_sequence = train_sequence[:-output_window] # todo: fix hack? -> din't think this through, looks like the last n sequences are to short, so I just remove them. Hackety Hack..\n",
    "    # looks like maybe solved\n",
    "    '''\n",
    "    #test_data = torch.FloatTensor(test_data).view(-1) \n",
    "    test_data = create_inout_sequences(test_data,input_window,output_window)\n",
    "    '''\n",
    "    test_data = test_data[:-output_window] # todo: fix hack?\n",
    "    '''\n",
    "    # shape with (block , sql_len , 2 )\n",
    "    return train_sequence.to(device),test_data.to(device)\n",
    "\n",
    "def get_batch(input_data, i , batch_size):\n",
    "\n",
    "    # batch_len = min(batch_size, len(input_data) - 1 - i) #  # Now len-1 is not necessary\n",
    "    batch_len = min(batch_size, len(input_data) - i)\n",
    "    data = input_data[ i:i + batch_len ]\n",
    "    input = torch.stack([item[0] for item in data]).view((input_window,batch_len,1))\n",
    "    # ( seq_len, batch, 1 ) , 1 is feature size\n",
    "    target = torch.stack([item[1] for item in data]).view((input_window,batch_len,1))\n",
    "    return input, target\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_data):\n",
    "    model.train() # Turn on the train mode \\o/\n",
    "    total_loss = 0.\n",
    "    start_time = time.time()\n",
    "\n",
    "    for batch, i in enumerate(range(0, len(train_data), batch_size)):  # Now len-1 is not necessary\n",
    "        # data and target are the same shape with (input_window,batch_len,1)\n",
    "        data, targets = get_batch(train_data, i , batch_size)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data,targets)\n",
    "        loss = criterion(output, targets)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.7)\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        log_interval = int(len(train_data) / batch_size / 5)\n",
    "        if batch % log_interval == 0 and batch > 0:\n",
    "            cur_loss = total_loss / log_interval\n",
    "            elapsed = time.time() - start_time\n",
    "            print('| epoch {:3d} | {:5d}/{:5d} batches | '\n",
    "                  'lr {:02.6f} | {:5.2f} ms | '\n",
    "                  'loss {:5.5f} | ppl {:8.2f}'.format(\n",
    "                    epoch, batch, len(train_data) // batch_size, scheduler.get_lr()[0],\n",
    "                    elapsed * 1000 / log_interval,\n",
    "                    cur_loss, math.exp(cur_loss)))\n",
    "            total_loss = 0\n",
    "            start_time = time.time()\n",
    "\n",
    "def plot_and_loss(eval_model, data_source,epoch):\n",
    "    eval_model.eval() \n",
    "    total_loss = 0.\n",
    "    test_result = torch.Tensor(0)    \n",
    "    truth = torch.Tensor(0)\n",
    "    with torch.no_grad():\n",
    "        # for i in range(0, len(data_source) - 1):\n",
    "        for i in range(len(data_source)):  # Now len-1 is not necessary\n",
    "            data, target = get_batch(data_source, i , 1) # one-step forecast\n",
    "            output = eval_model(data,target)            \n",
    "            total_loss += criterion(output, target).item()\n",
    "            test_result = torch.cat((test_result, output[-1].view(-1).cpu()), 0)\n",
    "            truth = torch.cat((truth, target[-1].view(-1).cpu()), 0)\n",
    "            \n",
    "    #test_result = test_result.cpu().numpy() -> no need to detach stuff.. \n",
    "    len(test_result)\n",
    "\n",
    "    pyplot.plot(test_result,color=\"red\")\n",
    "    pyplot.plot(truth[:500],color=\"blue\")\n",
    "    pyplot.plot(test_result-truth,color=\"green\")\n",
    "    pyplot.grid(True, which='both')\n",
    "    pyplot.axhline(y=0, color='k')\n",
    "    pyplot.savefig('graph/transformer-epoch%d.png'%epoch)\n",
    "    pyplot.close()\n",
    "    return total_loss / i\n",
    "\n",
    "\n",
    "# predict the next n steps based on the input data \n",
    "def predict_future(eval_model, data_source,steps):\n",
    "    eval_model.eval() \n",
    "    total_loss = 0.\n",
    "    test_result = torch.Tensor(0)    \n",
    "    truth = torch.Tensor(0)\n",
    "    data, _ = get_batch(data_source , 0 , 1)\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, steps):            \n",
    "            output = eval_model(data[-input_window:])\n",
    "            # (seq-len , batch-size , features-num)\n",
    "            # input : [ m,m+1,...,m+n ] -> [m+1,...,m+n+1]\n",
    "            data = torch.cat((data, output[-1:])) # [m,m+1,..., m+n+1]\n",
    "\n",
    "    data = data.cpu().view(-1)\n",
    "    \n",
    "    # I used this plot to visualize if the model pics up any long therm structure within the data.\n",
    "    pyplot.plot(data,color=\"red\")       \n",
    "    pyplot.plot(data[:input_window],color=\"blue\")    \n",
    "    pyplot.grid(True, which='both')\n",
    "    pyplot.axhline(y=0, color='k')\n",
    "    pyplot.savefig('graph/transformer-future%d.png'%steps)\n",
    "    pyplot.show()\n",
    "    pyplot.close()\n",
    "        \n",
    "\n",
    "def evaluate(eval_model, data_source):\n",
    "    eval_model.eval() # Turn on the evaluation mode\n",
    "    total_loss = 0.\n",
    "    eval_batch_size = 1000\n",
    "    with torch.no_grad():\n",
    "        # for i in range(0, len(data_source) - 1, eval_batch_size): # Now len-1 is not necessary\n",
    "        for i in range(0, len(data_source), eval_batch_size):\n",
    "            data, targets = get_batch(data_source, i,eval_batch_size)\n",
    "            output = eval_model(data, targets)            \n",
    "            total_loss += len(data[0]) * criterion(output, targets).cpu().item()\n",
    "    return total_loss / len(data_source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zsr/opt/anaconda3/envs/GR5242_Torch/lib/python3.11/site-packages/torch/nn/modules/transformer.py:282: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n",
      "/Users/zsr/opt/anaconda3/envs/GR5242_Torch/lib/python3.11/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([100, 10, 1])) that is different to the input size (torch.Size([100, 10, 250])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zsr/opt/anaconda3/envs/GR5242_Torch/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:384: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   1 |    62/  310 batches | lr 0.005000 | 348.05 ms | loss 0.75419 | ppl     2.13\n",
      "| epoch   1 |   124/  310 batches | lr 0.005000 | 314.97 ms | loss 0.21716 | ppl     1.24\n",
      "| epoch   1 |   186/  310 batches | lr 0.005000 | 292.60 ms | loss 0.19310 | ppl     1.21\n",
      "| epoch   1 |   248/  310 batches | lr 0.005000 | 334.84 ms | loss 0.15198 | ppl     1.16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zsr/opt/anaconda3/envs/GR5242_Torch/lib/python3.11/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([100, 700, 1])) that is different to the input size (torch.Size([100, 700, 250])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   1 | time: 107.02s | valid loss 0.25603 | valid ppl     1.29\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   2 |    62/  310 batches | lr 0.004513 | 385.77 ms | loss 0.06921 | ppl     1.07\n",
      "| epoch   2 |   124/  310 batches | lr 0.004513 | 365.55 ms | loss 0.10436 | ppl     1.11\n",
      "| epoch   2 |   186/  310 batches | lr 0.004513 | 374.40 ms | loss 0.06551 | ppl     1.07\n",
      "| epoch   2 |   248/  310 batches | lr 0.004513 | 386.02 ms | loss 0.08063 | ppl     1.08\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   2 | time: 122.49s | valid loss 0.46642 | valid ppl     1.59\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   3 |    62/  310 batches | lr 0.004287 | 405.42 ms | loss 0.04355 | ppl     1.04\n",
      "| epoch   3 |   124/  310 batches | lr 0.004287 | 386.87 ms | loss 0.06919 | ppl     1.07\n",
      "| epoch   3 |   186/  310 batches | lr 0.004287 | 376.63 ms | loss 0.04292 | ppl     1.04\n",
      "| epoch   3 |   248/  310 batches | lr 0.004287 | 395.63 ms | loss 0.05524 | ppl     1.06\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   3 | time: 125.08s | valid loss 0.05463 | valid ppl     1.06\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   4 |    62/  310 batches | lr 0.004073 | 408.95 ms | loss 0.02633 | ppl     1.03\n",
      "| epoch   4 |   124/  310 batches | lr 0.004073 | 408.05 ms | loss 0.04376 | ppl     1.04\n",
      "| epoch   4 |   186/  310 batches | lr 0.004073 | 374.22 ms | loss 0.02579 | ppl     1.03\n",
      "| epoch   4 |   248/  310 batches | lr 0.004073 | 475.37 ms | loss 0.03817 | ppl     1.04\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   4 | time: 130.06s | valid loss 0.05351 | valid ppl     1.05\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   5 |    62/  310 batches | lr 0.003869 | 251.68 ms | loss 0.01854 | ppl     1.02\n"
     ]
    }
   ],
   "source": [
    "train_data, val_data = get_data()\n",
    "model = TransAm().to(device)\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "lr = 0.005 \n",
    "#optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1, gamma=0.95)\n",
    "\n",
    "best_val_loss = float(\"inf\")\n",
    "epochs = 10 # The number of epochs\n",
    "best_model = None\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    epoch_start_time = time.time()\n",
    "    train(train_data)\n",
    "    if ( epoch % 5 == 0 ):\n",
    "        val_loss = plot_and_loss(model, val_data,epoch)\n",
    "        predict_future(model, val_data,200)\n",
    "    else:\n",
    "        val_loss = evaluate(model, val_data)\n",
    "   \n",
    "    print('-' * 89)\n",
    "    print('| end of epoch {:3d} | time: {:5.2f}s | valid loss {:5.5f} | valid ppl {:8.2f}'.format(epoch, (time.time() - epoch_start_time),\n",
    "                                     val_loss, math.exp(val_loss)))\n",
    "    print('-' * 89)\n",
    "\n",
    "    #if val_loss < best_val_loss:\n",
    "    #    best_val_loss = val_loss\n",
    "    #    best_model = model\n",
    "\n",
    "    scheduler.step() \n",
    "\n",
    "#src = torch.rand(input_window, batch_size, 1) # (source sequence length,batch size,feature number) \n",
    "#out = model(src)\n",
    "#\n",
    "#print(out)\n",
    "#print(out.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GR5242_Torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
